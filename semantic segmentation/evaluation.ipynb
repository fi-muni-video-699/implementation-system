{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from video699.video.annotated import get_videos, AnnotatedSampledVideo, AnnotatedSampledVideoScreenDetector, GEOSConvexQuadrangle, Point, ScreenABC\n",
    "from pathlib import Path\n",
    "from fastai.vision import load_learner, SegmentationLabelList, SegmentationItemList, DatasetType, defaults, Image\n",
    "import torch\n",
    "from functools import partial\n",
    "from fastai.metrics import dice\n",
    "from shapely.geometry import LineString\n",
    "from shapely.ops import split\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import scipy\n",
    "import pandas as pd\n",
    "# defaults.device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midpoint(pointA, pointB):\n",
    "    return ((pointA[0]+ pointB[0]) / 2, (pointA[1]+ pointB[1]) / 2)\n",
    "\n",
    "def get_coordinates(quadrangle):\n",
    "    squeezed = quadrangle.squeeze()\n",
    "    x = squeezed[:, 0]\n",
    "    y = squeezed[:, 1]\n",
    "    top_left = (x+y).argmin()\n",
    "    top_right = (max(y)-y + x).argmax()\n",
    "    bottom_left = (max(x)-x + y).argmax()\n",
    "    bottom_right = (x+y).argmax()\n",
    "    return {'top_left': squeezed[top_left], \n",
    "            'top_right': squeezed[top_right], \n",
    "            'bottom_right': squeezed[bottom_right], \n",
    "            'bottom_left': squeezed[bottom_left]}\n",
    "        \n",
    "def draw_polygon(polygon, image):\n",
    "    copy = image.copy()\n",
    "    return cv2.fillConvexPoly(copy, polygon, 100)\n",
    "\n",
    "def draw_polygons(polygons, image, show=True):\n",
    "    copy=image.copy()\n",
    "    # Visualization\n",
    "    if len(polygons) == 0:\n",
    "        if show:\n",
    "            plt.imshow(copy)\n",
    "            plt.show()\n",
    "    else:\n",
    "        for polygon in polygons:\n",
    "            copy = cv2.fillConvexPoly(copy, polygon, 100)\n",
    "        if show:\n",
    "            plt.imshow(copy)\n",
    "            plt.show()\n",
    "    return copy\n",
    "\n",
    "class SegLabelListCustom(SegmentationLabelList):\n",
    "    def open(self, fn): return open_mask(fn, div=False, convert_mode='L')\n",
    "\n",
    "class SegItemListCustom(SegmentationItemList):\n",
    "    _label_cls = SegLabelListCustom\n",
    "\n",
    "def acc(input, targs):\n",
    "    \"Accuracy.\"\n",
    "    targs = targs.squeeze(1)\n",
    "    return (input.argmax(dim=1)==targs).float().mean()\n",
    "\n",
    "# iou = partial(dice, iou=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/mnt/c/Users/mikul/Desktop/BP/implementation-system\")\n",
    "videos = get_videos()\n",
    "keys = list(videos.keys())\n",
    "videos_list = [videos[key] for key in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastAIVideoScreen(ScreenABC):\n",
    "    def __init__(self, frame, screen_index, coordinates):\n",
    "        self._frame = frame\n",
    "        self._screen_index = screen_index\n",
    "        self._coordinates = coordinates\n",
    "\n",
    "    @property\n",
    "    def frame(self):\n",
    "        return self._frame\n",
    "\n",
    "    @property\n",
    "    def coordinates(self):\n",
    "        return self._coordinates\n",
    "    \n",
    "class FastAIScreenDetector:\n",
    "    def __init__(self, path, methods):\n",
    "        self.learner = load_learner(path=path, bs=4)\n",
    "        self.methods=methods\n",
    "    \n",
    "    def detect(self, frame):\n",
    "        # Semantic segmentation\n",
    "        image = cv2.cvtColor(frame.image, cv2.COLOR_RGBA2RGB)\n",
    "        tensor = torch.from_numpy(np.transpose(image, (2, 0, 1)))\n",
    "        tensor = Image(tensor.to(torch.float32) / 255)\n",
    "        pred = self.learner.predict(tensor)\n",
    "        predicted_numpy = np.squeeze(np.transpose(pred[1].numpy(), (1, 2, 0))).astype('uint8')\n",
    "        shape = predicted_numpy.shape\n",
    "        predicted_resized = cv2.resize(predicted_numpy, dsize=(shape[1]*2, shape[0]*2))\n",
    "        # Screen retrieval (Post processing)\n",
    "        geos_quadrangles = approximate(predicted_resized, methods=self.methods)\n",
    "    \n",
    "        # Create screens (System Data Types)\n",
    "        return [FastAIVideoScreen(frame, screen_index, quadrangle) for screen_index, quadrangle in enumerate(geos_quadrangles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(screenA, screenB):\n",
    "    intersection = screenA.coordinates.intersection_area(screenB.coordinates)\n",
    "    union = screenA.coordinates.union_area(screenB.coordinates)\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of single video predictions with total number of screens + iou\n",
    "* dice, accruacy metrics can be added later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_video_statistics(videos, actual_detector):\n",
    "    sizes = []\n",
    "    ratios = []\n",
    "    for video in videos:\n",
    "        size, ratio = single_video_statistics(video, actual_detector)\n",
    "        sizes.extend(size)\n",
    "        ratios.extend(ratio)\n",
    "    return sizes, ratios\n",
    "\n",
    "def single_video_statistics(video, actual_detector):\n",
    "    sizes = []\n",
    "    ratios = []\n",
    "    for frame in tqdm(video):\n",
    "        actual_screens = actual_detector.detect(frame)\n",
    "        for screen in actual_screens:\n",
    "            sizes.append(screen.coordinates.area)\n",
    "            ratios.append(screen.coordinates.height / screen.coordinates.width)\n",
    "    return sizes, ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_videos_eval(videos, actual_detector, pred_detector):\n",
    "    all_wrong_screen_count_frames, all_ious, all_really_bad_ious = [], [] ,[]\n",
    "    for video in videos:\n",
    "        wrong_screen_count_frames, ious, really_bad_ious = single_video_eval(video, actual_detector, pred_detector)\n",
    "        all_wrong_screen_count_frames.extend(wrong_screen_count_frames)\n",
    "        all_ious.extend(ious)\n",
    "        all_really_bad_ious.extend(really_bad_ious)\n",
    "    return all_wrong_screen_count_frames, all_ious, all_really_bad_ious\n",
    "\n",
    "def single_video_eval(video, actual_detector, pred_detector):\n",
    "    wrong_screen_count_frames = []\n",
    "    ious = []\n",
    "    really_bad_ious = []\n",
    "    for frame in tqdm(video):\n",
    "        frame_ious = []\n",
    "        actual_screens = sorted(actual_detector.detect(frame), key=lambda screen: screen.coordinates.top_left[0])\n",
    "        pred_screens = sorted(pred_detector.detect(frame), key=lambda screen: screen.coordinates.top_left[0])\n",
    "        if len(actual_screens) != len(pred_screens):\n",
    "            wrong_screen_count_frames.append(frame)\n",
    "            # Think about what to do !!!\n",
    "        else:\n",
    "            for screenA, screenB in zip(actual_screens, pred_screens):\n",
    "                score = iou(screenA, screenB)\n",
    "                frame_ious.append(score)\n",
    "        \n",
    "        if len(frame_ious) > 0:\n",
    "            score = np.array(frame_ious).mean()\n",
    "        else:\n",
    "            score = np.nan\n",
    "        ious.append(score)\n",
    "        if score < 0.92:\n",
    "            really_bad_ious.append(frame)\n",
    "    return wrong_screen_count_frames, ious, really_bad_ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legend_without_duplicate_labels(ax):\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique = [(h, l) for i, (h, l) in enumerate(zip(handles, labels)) if l not in labels[:i]]\n",
    "    ax.legend(*zip(*unique))\n",
    "    \n",
    "def single_frame_visualization(frame, actual_detector, pred_detector, methods=['erose_dilate', 'ratio_split']):\n",
    "    actual_screens = sorted(actual_detector.detect(frame), key=lambda screen: screen.coordinates.top_left[0])\n",
    "    pred_screens = sorted(pred_detector.detect(frame), key=lambda screen: screen.coordinates.top_left[0])\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 4),sharex='row', sharey='row')\n",
    "    fig.tight_layout()\n",
    "    for screen in actual_screens:\n",
    "        polygon = screen.coordinates._polygon\n",
    "        axes[0].set_title(\"Actual\")\n",
    "        axes[0].plot(*polygon.exterior.xy, c='tab:orange')\n",
    "        axes[2].plot(*polygon.exterior.xy, c='tab:orange', label=\"Actual\")\n",
    "        \n",
    "    for screen in pred_screens:\n",
    "        polygon = screen.coordinates._polygon\n",
    "        axes[1].set_title(\"Prediction\")\n",
    "        axes[1].plot(*polygon.exterior.xy, c='tab:blue')\n",
    "        axes[2].plot(*polygon.exterior.xy, c='tab:blue', label=\"Predicted\")\n",
    "    \n",
    "    axes[2].set_title(\"Combined\")\n",
    "    legend_without_duplicate_labels(axes[2])\n",
    "    \n",
    "    axes[3].set_title(\"Original\")\n",
    "    axes[3].imshow(frame.image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_frames_visualization(frames, actual_detector, pred_detector):\n",
    "    for frame in frames:\n",
    "        single_frame_visualization(frame, actual_detector, pred_detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_approx(contours, lower_bound, upper_bound, factors):\n",
    "    quadrangles = []\n",
    "    for cnt in contours:\n",
    "        if upper_bound > cv2.contourArea(cnt) > lower_bound:\n",
    "            for factor in factors:\n",
    "                epsilon = factor*cv2.arcLength(cnt, True)\n",
    "                polygon = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "                if polygon.shape[0] == 4:\n",
    "                    quadrangles.append(polygon)\n",
    "                    break\n",
    "    return quadrangles\n",
    "    \n",
    "def approximate(pred, methods):\n",
    "    if 'base' in methods.keys():\n",
    "        quadrangles = approximate_baseline(pred, **methods['base'])\n",
    "        \n",
    "    if 'erose_dilate' in methods.keys():\n",
    "        erose_dilate_quadrangles = approximate_erose_dilate(pred, **methods['erose_dilate'])\n",
    "        quadrangles = erose_dilate_quadrangles if len(erose_dilate_quadrangles) > len(quadrangles) else quadrangles\n",
    "        \n",
    "    if 'ratio_split' in methods.keys():\n",
    "        quadrangles = approximate_ratio_split(quadrangles, **methods['ratio_split'])\n",
    "        \n",
    "    else:\n",
    "        quadrangles = [GEOSConvexQuadrangle(**get_coordinates(quadrangle)) for quadrangle in quadrangles]\n",
    "    \n",
    "    return quadrangles\n",
    "    \n",
    "def approximate_baseline(pred, lower_bound, upper_bound, factors):\n",
    "    contours, _ = cv2.findContours(pred, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    quadrangles = contour_approx(contours, lower_bound, upper_bound, factors)\n",
    "    return quadrangles\n",
    "\n",
    "def approximate_erose_dilate(pred, lower_bound, upper_bound, iterations, factors):\n",
    "    # TODO contours rozdelit po jednom erose a dilatace.\n",
    "    erosed = cv2.erode(pred, None, iterations=iterations)\n",
    "    contours, _ = cv2.findContours(erosed, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    quadrangles = contour_approx(contours, lower_bound, upper_bound, factors)\n",
    "    erosed_dilated_quadrangles = []\n",
    "    for quadrangle in quadrangles:\n",
    "        zeros = np.zeros(pred.shape, dtype='uint8')\n",
    "        erosed_quadrangle = draw_polygon(quadrangle, zeros)\n",
    "        dilated_quadrangle = cv2.dilate(erosed_quadrangle, None, iterations=iterations)\n",
    "        contours, _ = cv2.findContours(dilated_quadrangle, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        erosed_dilated_quadrangles.extend(contour_approx(contours, lower_bound, upper_bound, factors))\n",
    "    return erosed_dilated_quadrangles\n",
    "\n",
    "def approximate_ratio_split(quadrangles, lower_ratio_bound, upper_ratio_bound):  \n",
    "    ratio_split_quadrangles = []\n",
    "    for quadrangle in quadrangles:\n",
    "        geos_quadrangle = GEOSConvexQuadrangle(**get_coordinates(quadrangle))\n",
    "        \n",
    "        if lower_ratio_bound < geos_quadrangle.height / geos_quadrangle.width < upper_ratio_bound or geos_quadrangle.area < 80000:\n",
    "            ratio_split_quadrangles.append(geos_quadrangle)\n",
    "            continue\n",
    "            \n",
    "        if not lower_ratio_bound < geos_quadrangle.height / geos_quadrangle.width:\n",
    "            upper_midpoint = midpoint(geos_quadrangle.top_left, geos_quadrangle.top_right)\n",
    "            lower_midpoint = midpoint(geos_quadrangle.bottom_left, geos_quadrangle.bottom_right)\n",
    "            line = LineString([upper_midpoint, lower_midpoint])\n",
    "            result = split(geos_quadrangle._polygon, line)\n",
    "\n",
    "        elif not geos_quadrangle.height / geos_quadrangle.width < upper_ratio_bound:\n",
    "            left_midpoint = midpoint(geos_quadrangle.top_left, geos_quadrangle.bottom_left)\n",
    "            right_midpoint = midpoint(geos_quadrangle.top_right, geos_quadrangle.bottom_right)\n",
    "            line = LineString([left_midpoint, right_midpoint])\n",
    "            result = split(geos_quadrangle._polygon, line)\n",
    "            \n",
    "        for res in result:\n",
    "            x, y = res.exterior.coords.xy\n",
    "            coords = np.array([list(a) for a in zip(x,y)])\n",
    "            ratio_split_quadrangles.append(GEOSConvexQuadrangle(**get_coordinates(coords)))\n",
    "    return ratio_split_quadrangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_parameters = {'lower_bound': 30000, 'upper_bound': 200000, 'factors': [0.1, 0.01]}\n",
    "erose_dilate_parameters = {'lower_bound': 30000, 'upper_bound': 200000, 'factors': [0.1, 0.01], 'iterations': 40}\n",
    "ratio_split_baseline = {\"lower_ratio_bound\": 0.7, 'upper_ratio_bound': 1.5}\n",
    "\n",
    "actual_detector = AnnotatedSampledVideoScreenDetector()\n",
    "# pred_detector = FastAIScreenDetector(path, methods={'base': baseline_parameters})\n",
    "# pred_detector_erose = FastAIScreenDetector(path, methods={'erose_dilate': baseline_parameters})\n",
    "# pred_detector_ratio = FastAIScreenDetector(path, methods={'ratio_split': ratio_split_baseline})\n",
    "pred_detector_all = FastAIScreenDetector(path, methods={'base': baseline_parameters,\n",
    "                                                        'erose_dilate': erose_dilate_parameters,\n",
    "                                                        'ratio_split': ratio_split_baseline\n",
    "                                                       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next ideas\n",
    "* Find videos with worst IoU automatically\n",
    "* Find videos with most screen misses automatically\n",
    "* Find individual frames with worst IoU\n",
    "* Investigate effect of different post-processing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wrong_count, ious, really_bad_ious = all_videos_eval(videos_list, actual_detector, pred_detector_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually optimized\n",
    "print(f\"Size of test data: {len(ious)}\")\n",
    "print(f\"Number of examples with wrong number of screens: {len(wrong_count)}\") \n",
    "print(f\"Ratio of examples with wrong number of screens to all examples: {len(wrong_count) / len(ious)}\")\n",
    "print(f\"Mean iou: {np.nanmean(ious)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames_visualization(wrong_count, actual_detector, pred_detector_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_frames_visualization(really_bad_ious, actual_detector, pred_detector_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging and checking the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = load_learner(path)\n",
    "methods = []\n",
    "\n",
    "frame = really_bad_ious[0]\n",
    "image = cv2.cvtColor(frame.image, cv2.COLOR_RGBA2RGB)\n",
    "tensor = torch.from_numpy(np.transpose(image, (2, 0, 1)))\n",
    "tensor = Image(tensor.to(torch.float32) / 255)\n",
    "pred = learner.predict(tensor)\n",
    "predicted_numpy = np.squeeze(np.transpose(pred[1].numpy(), (1, 2, 0))).astype('uint8')\n",
    "shape = predicted_numpy.shape\n",
    "predicted_resized = cv2.resize(predicted_numpy, dsize=(shape[1]*2, shape[0]*2))\n",
    "# Screen retrieval (Post processing)\n",
    "geos_quadrangles = approximate(predicted_resized, methods=methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(predicted_resized)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quadrangles = []\n",
    "lower_bound=30000\n",
    "upper_bound=200000\n",
    "contours, _ = cv2.findContours(predicted_resized, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "factors = [0.1, 0.01]\n",
    "for cnt in contours:\n",
    "    if upper_bound > cv2.contourArea(cnt) > lower_bound:\n",
    "        for factor in factors:\n",
    "            epsilon = factor*cv2.arcLength(cnt, True)\n",
    "            polygon = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "            if polygon.shape[0] == 4:\n",
    "                zeros = np.zeros(predicted_resized.shape, dtype='uint8')\n",
    "                draw_polygons([polygon], zeros, show=True)\n",
    "                quadrangles.append(polygon)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get statistics about dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes, ratios = all_video_statistics(videos_list, actual_detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_interval(data, ndigits=0):\n",
    "    round_n_decimals = partial(round, ndigits=ndigits)\n",
    "    data = np.array(data)\n",
    "    mean, std = data.mean(), data.std()\n",
    "    return map(round_n_decimals, scipy.stats.norm.interval(0.95, loc=mean, scale=std)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_area, upper_area = get_confidence_interval(sizes)\n",
    "lower_area, upper_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lower_ratio, upper_ratio = get_confidence_interval(ratios, ndigits=3)\n",
    "lower_ratio, upper_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use random search with hyper-opt\n",
    "lower_bounds = [30000, 36050, 40000]\n",
    "upper_bounds = [99229, 125000, 175000, 200000]\n",
    "factor_list = [[0.1, 0.01], [0.1], [0.01], [0.01, 0.02]]\n",
    "\n",
    "all_settings = [(lower, upper, factors) for lower in lower_bounds for upper in upper_bounds for factors in factor_list]\n",
    "results = pd.DataFrame(columns=['lower_bound', 'upper_bound', 'factors', 'mean_iou', 'number_of_wrong_screens'])\n",
    "\n",
    "for setting in all_settings:\n",
    "    parameters = {'lower_bound': setting[0], 'upper_bound': setting[1], 'factors': setting[2]}\n",
    "    pred_detector_grid_search = FastAIScreenDetector(path, methods={'base': parameters})\n",
    "    wrong_count, ious, really_bad_ious = all_videos_eval(videos_list, actual_detector, pred_detector_grid_search)\n",
    "    results.loc[len(results)] = [setting[0], setting[1], setting[2], np.nanmean(ious), len(wrong_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erose_dilate_parameters = {'lower_bound': 30000, 'upper_bound': 200000, 'factors': [0.1, 0.01], 'iterations': 40}\n",
    "# ratio_split_baseline = {\"lower_ratio_bound\": 0.7, 'upper_ratio_bound': 1.5}\n",
    "# iterations = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "# lower_ratio_bound = [0.648, 0.7, 0.8, 0.9]\n",
    "# upper_ratio_bound = [0.883, 1.0, 1.25, 1.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple images in one batch to better use GPU parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = SegItemListCustom.from_folder(path/'video699/video/annotated', recurse=True)\n",
    "\n",
    "# learner.data.add_test(test, tfms=None, tfm_y=False)\n",
    "\n",
    "# preds = learner.get_preds(DatasetType.Test)[0].squeeze().numpy()\n",
    "# preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.bitwise_and(zeros.astype(bool), prediction.astype(bool)).sum() / np.bitwise_or(zeros.astype(bool), prediction.astype(bool)).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
